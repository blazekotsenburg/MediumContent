{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from models.prompt import Prompt\n",
    "from dotenv import load_dotenv\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import transformers\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "import random, hashlib, numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import norm\n",
    "\n",
    "from common.orchestrator import Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#Same backbone the paper used: ViT-L/14\n",
    "TOKENIZER = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "TEXT_MODEL = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")#.to(DEVICE)\n",
    "TEXT_MODEL.eval()\n",
    "\n",
    "@dataclass\n",
    "class APOResult:\n",
    "    sensitive_prompt: str\n",
    "    best_prompt: str\n",
    "    best_score: float\n",
    "    queries: int\n",
    "\n",
    "load_dotenv(dotenv_path=\"/Users/blazekotsenburg/Documents/Source/Repos/MediumContent/AdversarialML/MJA/.env\")\n",
    "PATH_METAPHOR_SYS_PROMPT=os.getenv(\"PATH_METAPHOR_SYS_PROMPT\")\n",
    "PATH_CONTEXT_SYS_PROMPT=os.getenv(\"PATH_CONTEXT_SYS_PROMPT\")\n",
    "PATH_ADV_SYS_PROMPT=os.getenv(\"PATH_ADV_SYS_PROMPT\")\n",
    "\n",
    "PATH_METAPHOR_USR_PROMPT=os.getenv(\"PATH_METAPHOR_USR_PROMPT\")\n",
    "PATH_CONTEXT_USR_PROMPT=os.getenv(\"PATH_CONTEXT_USR_PROMPT\")\n",
    "PATH_ADV_USR_PROMPT=os.getenv(\"PATH_ADV_USR_PROMPT\")\n",
    "\n",
    "# p = Prompt.load_from_file(file_path=PATH_METAPHOR_PROMPT)\n",
    "# print(p.render())\n",
    "\n",
    "sys_prompt_metaphor   = Prompt.load_from_file(file_path=PATH_METAPHOR_SYS_PROMPT)\n",
    "sys_prompt_context    = Prompt.load_from_file(file_path=PATH_CONTEXT_SYS_PROMPT)\n",
    "sys_prompt_adverarial = Prompt.load_from_file(file_path=PATH_ADV_SYS_PROMPT)\n",
    "\n",
    "usr_prompt_metaphor     = Prompt.load_from_file(file_path=PATH_METAPHOR_USR_PROMPT)\n",
    "usr_prompt_context      = Prompt.load_from_file(file_path=PATH_CONTEXT_USR_PROMPT)\n",
    "usr_prompt_adversarial  = Prompt.load_from_file(file_path=PATH_ADV_USR_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llama_3_8b = Orchestrator(model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metaphors(x_sen: str, N: int) -> List[str]:\n",
    "    metaphors = []\n",
    "    for _ in range(N):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": sys_prompt_metaphor.render()},\n",
    "            {\"role\": \"user\", \"content\": usr_prompt_metaphor.render(sen_content=x_sen)}\n",
    "        ]\n",
    "        metaphors.append(llama_3_8b.invoke(messages=messages))\n",
    "    return metaphors\n",
    "\n",
    "def contexts(x_sen: str, metaphor: str, M: int) -> List[str]:\n",
    "    contexts = []\n",
    "    for _ in range(M):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": sys_prompt_context.render()},\n",
    "            {\"role\": \"user\", \"content\": usr_prompt_context.render(sen_content=x_sen, metaphor=metaphor)}\n",
    "        ]\n",
    "        contexts.append(llama_3_8b.invoke(messages=messages))\n",
    "    return contexts\n",
    "\n",
    "def adversarial(x_sen: str, metaphor:str, ctx: str) -> List[str]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt_adverarial.render()},\n",
    "        {\"role\": \"user\", \"content\": usr_prompt_adversarial.render(sen_content=x_sen, metaphor=metaphor, artistic_context=ctx)}\n",
    "    ]\n",
    "\n",
    "    return llama_3_8b.invoke(messages=messages)\n",
    "\n",
    "def t2i_generate(prompt: str) -> str:\n",
    "    \"\"\"Call your T2I model here (stub returns prompt as 'image').\"\"\"\n",
    "    return f\"IMAGE({prompt})\"\n",
    "\n",
    "def safety_filter(image: str) -> bool:\n",
    "    \"\"\"Return True if NOT blocked (0 in indicator).\"\"\"\n",
    "    return random.random() > 0.3  # 70 % chance to pass\n",
    "\n",
    "def image_text_similarity(image: str, x_sen: str) -> float:\n",
    "    \"\"\"Return similarity score (CLIP, etc.).\"\"\"\n",
    "    # hash-based pseudo‑similarity for demo only\n",
    "    h_img = int(hashlib.sha256(image.encode()).hexdigest(), 16) % 1000\n",
    "    h_txt = int(hashlib.sha256(x_sen.encode()).hexdigest(), 16) % 1000\n",
    "    return 1 - abs(h_img - h_txt) / 1000\n",
    "\n",
    "# ----------------------- LHS (loose implementation) --------------------- #\n",
    "def lhs_split(items: List[str], n_obs: int) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Simple Latin‑Hypercube‑like split: shuffle then take first n_obs.\"\"\"\n",
    "    shuffled = items.copy()\n",
    "    random.shuffle(shuffled)\n",
    "    return shuffled[:n_obs], shuffled[n_obs:]\n",
    "\n",
    "# --------------- Ground‑truth evaluation for a prompt ------------------- #\n",
    "def ground_truth(prompt: str, x_sen: str) -> float:\n",
    "    \"\"\"Return score = similarity * pass‑indicator (0 if blocked).\"\"\"\n",
    "    img = t2i_generate(prompt)\n",
    "    if not safety_filter(img):\n",
    "        return 0.0\n",
    "    return image_text_similarity(img, x_sen)\n",
    "\n",
    "def embed(prompt: str, dim: int = 256) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a text prompt into a 768-dim CLIP embedding (NumPy, CPU).\n",
    "    Works with Hugging Face 'openai/clip-vit-large-patch14'.\n",
    "    \"\"\"\n",
    "    # 1. Tokenise; returns a dict of tensors\n",
    "    tokens = TOKENIZER(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",   # CLIP expects exactly 77 tokens\n",
    "        max_length=77,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # 2. Forward pass through CLIP text encoder\n",
    "    outputs = TEXT_MODEL(**tokens)\n",
    "\n",
    "    # 3. Take the *pooled* text embedding (CLS token at position 0)\n",
    "    text_emb = outputs.last_hidden_state[:, 0, :]   # shape [1, 768]\n",
    "\n",
    "    # 4. L2-normalise so cosine-sim == dot product\n",
    "    text_emb = torch.nn.functional.normalize(text_emb, p=2, dim=-1)\n",
    "\n",
    "    # 5. Move to CPU & flatten → NumPy row\n",
    "    return text_emb.squeeze(0).cpu().numpy()\n",
    "\n",
    "def run_apo(x_sen: str) -> APOResult:\n",
    "    for met in metaphors(x_sen=x_sen, N=N):\n",
    "        for ctx in contexts(x_sen=x_sen, metaphor=met, M=M):\n",
    "            candidates.append(adversarial(x_sen=x_sen, metaphor=met, ctx=ctx))\n",
    "\n",
    "    # 2) Initial observation / candidate split\n",
    "    obs_prompts, can_prompts = lhs_split(candidates, min(N_OBS, len(candidates)))\n",
    "    obs_scores = [ground_truth(p, x_sen) for p in obs_prompts]\n",
    "\n",
    "    # Early success check\n",
    "    best_idx = int(np.argmax(obs_scores))\n",
    "    best_prompt, best_score = obs_prompts[best_idx], obs_scores[best_idx]\n",
    "    if best_score >= SIM_THRESHOLD:\n",
    "        return APOResult(x_sen, best_prompt, best_score, len(obs_prompts))\n",
    "\n",
    "    no_improve = 0\n",
    "    total_queries = len(obs_prompts)\n",
    "\n",
    "    # --- Bayesian optimisation loop --- #\n",
    "    while can_prompts:\n",
    "        # Feature extraction + dimensionality reduction\n",
    "        X_emb = np.array([embed(p) for p in obs_prompts])\n",
    "        X_emb = PCA(n_components=min(50, X_emb.shape[1])).fit_transform(X_emb)\n",
    "\n",
    "        pca = PCA(n_components=50).fit(X_emb)   #   <-- fit ONCE\n",
    "        X_emb_reduced = pca.transform(X_emb)    #   <-- train GPR on this\n",
    "\n",
    "        # Fit surrogate\n",
    "        gpr = GaussianProcessRegressor(kernel=Matern(nu=2.5))\n",
    "        gpr.fit(X_emb_reduced, obs_scores)\n",
    "\n",
    "        # Predict μ, σ for candidates\n",
    "        mu, sigma = [], []\n",
    "        for p in can_prompts:\n",
    "            # vec = PCA(n_components=min(50, X_emb.shape[1])).fit_transform(embed(p).reshape(1, -1))\n",
    "            vec = embed(p).reshape(1, -1)       # CLIP → 768-dim\n",
    "            vec = pca.transform(vec)  \n",
    "            m, s = gpr.predict(vec, return_std=True)\n",
    "            mu.append(m.item())\n",
    "            sigma.append(s.item())\n",
    "\n",
    "        mu, sigma = np.array(mu), np.array(sigma)\n",
    "        Z = (mu - best_score) / (sigma + 1e-9)\n",
    "        ei = (mu - best_score) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "\n",
    "        # Select best EI candidate\n",
    "        best_can_idx = int(np.argmax(ei))\n",
    "        next_prompt = can_prompts.pop(best_can_idx)\n",
    "\n",
    "        # Real query\n",
    "        next_score = ground_truth(next_prompt, x_sen)\n",
    "        total_queries += 1\n",
    "\n",
    "        # Update observation sets\n",
    "        obs_prompts.append(next_prompt)\n",
    "        obs_scores.append(next_score)\n",
    "\n",
    "        # Check improvement / success\n",
    "        if next_score > best_score:\n",
    "            best_score, best_prompt = next_score, next_prompt\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if best_score >= SIM_THRESHOLD:\n",
    "            break\n",
    "        if no_improve >= EARLY_STOP_ROUNDS:\n",
    "            break\n",
    "        \n",
    "    return APOResult(x_sen, best_prompt, best_score, total_queries)\n",
    "    \n",
    "    \n",
    "# for idx, row in df.iterrows():\n",
    "#     rows[\"idx\"] = idx\n",
    "\n",
    "#     sen_content = row[\"content\"]\n",
    "#     rows[\"sen_content\"] = sen_content\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": sys_prompt_metaphor.render()},\n",
    "#         {\"role\": \"user\", \"content\": usr_prompt_metaphor.render(sen_content=sen_content)}\n",
    "#     ]\n",
    "#     print(messages)\n",
    "\n",
    "#     metaphor_result = llama_3_8b.invoke(messages=messages)\n",
    "#     rows[\"metaphor\"].append(metaphor_result)\n",
    "\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": sys_prompt_context.render()},\n",
    "#         {\"role\": \"user\", \"content\": usr_prompt_context.render(sen_content=sen_content, metaphor=metaphor_result)}\n",
    "#     ]\n",
    "#     print(messages)\n",
    "\n",
    "#     context_result = llama_3_8b.invoke(messages=messages)\n",
    "#     rows[\"context\"].append(context_result)\n",
    "\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": sys_prompt_adverarial.render()},\n",
    "#         {\"role\": \"user\", \"content\": usr_prompt_adversarial.render(sen_content=sen_content, metaphor=metaphor_result, artistic_context=context_result)}\n",
    "#     ]\n",
    "\n",
    "#     adversarial_result = llama_3_8b.invoke(messages=messages)\n",
    "#     rows[\"adversaril\"].append(adversarial_result)\n",
    "\n",
    "# df_candidates = pd.DataFrame.from_dict(data=rows)\n",
    "# df_candidates.to_csv(\"data_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/blazekotsenburg/Documents/Source/Repos/MediumContent/AdversarialML/MJA/data/mja_dataset_2.csv\")\n",
    "\n",
    "rows={\n",
    "    \"idx\": [],\n",
    "    \"sen_content\": [],\n",
    "    \"metaphor\":[],\n",
    "    \"context\": [],\n",
    "    \"adversarial\": []\n",
    "}\n",
    "\n",
    "N                 = 7\n",
    "M                 = 6\n",
    "N_OBS             = 8\n",
    "EARLY_STOP_ROUNDS = 7    # R in the paper\n",
    "SIM_THRESHOLD     = 0.85  # τ in the paper\n",
    "\n",
    "candidates=[]\n",
    "for idx, row in df.iterrows():\n",
    "    x_sen = row[\"content\"]\n",
    "    run_apo(x_sen=x_sen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mja-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
